# Main plotting script
import argparse
import os
from glob import glob
import re

import numpy as np
from tqdm import tqdm

# Argument parser is moved to __main__ method, unlike in other scripts
# (This is to have list of available functions)

def _load_cm_data_tsne(input_file):
    """
    Read arrays from a file stored by train_siamese_cm.py where input_file
    is a numpy "savez" array with bonafide samples, spoof samples and systems
    used to create spoof systems.
    """
    data = np.load(input_file)

    bonafide_samples = data["bonafide"]
    spoof_samples = data["spoof"]
    spoof_systems = data["spoof_systems"].tolist()

    # Concatenate everything into one array/list.
    # Add bonafide labels to labels array
    systems = (["Bonafide"] * len(bonafide_samples)) + spoof_systems
    all_samples = np.concatenate((bonafide_samples, spoof_samples), axis=0)

    return all_samples, systems


def plot_embedding(remaining_args):
    # Import function-specific-functions here to avoid
    # making script too slow for everyone else
    from utils.plot_utils import plot_tsne

    parser = argparse.ArgumentParser("Plot TSNE plots")
    parser.add_argument("type", choices=["cm"], type=str, help="What type of data we have.")
    parser.add_argument("input", help="Input file where to read vectors from")
    parser.add_argument("output", help="Location for output figure")
    parser.add_argument("--use-umap", action="store_true", help="Use UMAP instead of tSNE")
    args = parser.parse_args(remaining_args)

    # Load data into one big arrays.
    # These functions (depending on the type) should return two variables:
    #   - ndarray of NxD, each row being one vector (point) to be plotted
    #   - legends (List of str), class for each point for plotting
    all_vectors = None
    legends = None
    if args.type == "cm":
        all_vectors, legends = _load_cm_data_tsne(args.input)

    # Sort so we have same order in different runs
    unique_classes = sorted(list(set(legends)))

    # Split into list of arrays
    arrays = []
    for unique_class in unique_classes:
        bool_mask = [legend == unique_class for legend in legends]
        arrays.append(all_vectors[np.array(bool_mask)])

    # Finally, plot the thing
    plot_tsne(arrays, unique_classes, filename=args.output, use_umap=args.use_umap)


def plot_score_animation(remaining_args):
    """
    Read bunch of asv/cm score-files, and create an animation
    of how the scores change over the training run.

    Note: This assumes score directories generated by the other scripts,
          e.g. train_xvector_asv.py and train_siamese_cm.py.
          The stored score files have name of form
          [experiment_name]_updates_[int]_[asv/cm]_model.txt.[asv/cm]_scores
    """
    from utils.plot_utils import (
        plot_joint_score_scatter_animation,
        plot_joint_score_training_progress,
        plot_joint_det_animation,
        plot_joint_score_scatter_animation_snapshots,
        plot_joint_det_change
    )
    from utils.score_loading import load_scorefile_and_split_scores, load_scorefile_and_split_to_arrays
    from utils import tdcf

    parser = argparse.ArgumentParser("Plot asv/cm score animation")
    parser.add_argument("asv_score_dir", type=str, help="Directory where ASV scores are stored")
    parser.add_argument("cm_score_dir", type=str, help="Directory where CM scores are stored")
    parser.add_argument("experiment_name", type=str, help="Experiment name (the prefix of the files)")
    parser.add_argument("output", help="Location where the output should go")
    parser.add_argument("--metrics", action="store_true", help="Include metrics (EER and t-DCF in the plot")
    parser.add_argument("--quick", action="store_true", help="Do not render videos")
    args = parser.parse_args(remaining_args)

    # Example filename of one scorefile for asv
    # joint_rl_asv_11_cm_3_updates_0_asv_model.txt.asv_scores
    asv_files = glob(os.path.join(args.asv_score_dir, args.experiment_name) + "_updates_*asv_scores")
    cm_files = glob(os.path.join(args.cm_score_dir, args.experiment_name) + "_updates_*cm_scores")

    assert len(asv_files) > 0, "Could not find any ASV scorefiles under this experiment name"
    assert len(cm_files) > 0, "Could not find any CM scorefiles under this experiment name"
    assert len(asv_files) == len(cm_files), "Found un-equal amount of ASV and CM scorefiles"

    # Sort by how far files were in the training
    asv_files_updates = [int(re.findall("updates\_([0-9]*)", filepath)[0]) for filepath in asv_files]
    cm_files_updates = [int(re.findall("updates\_([0-9]*)", filepath)[0]) for filepath in cm_files]

    # Combine filenames with the number of updates and sort
    asv_files = list(sorted(zip(asv_files_updates, asv_files), key=lambda x: x[0]))
    cm_files = list(sorted(zip(cm_files_updates, cm_files), key=lambda x: x[0]))

    # Read scores from all files and gather them up to lists
    # for the plot call
    asv_scores_list = []
    cm_scores_list = []
    asv_is_target_list = []
    cm_is_target_list = []
    titles_list = []

    # Gather all EERs and tDCFs into one one lists for plotting later
    num_updates = []
    cm_eers = []
    asv_eers = []
    min_tdcfs = []

    # Read scores of all files ready for plotting
    for (asv_updates, asv_file), (cm_updates, cm_file) in tqdm(zip(asv_files, cm_files), desc="load", total=len(asv_files)):
        # Check that asv and cm file match
        assert asv_updates == cm_updates, "Something went wonky with matching ASV and CM files"

        asv_is_target, asv_scores, asv_systems = load_scorefile_and_split_to_arrays(asv_file)
        asv_scores = asv_scores.astype(np.float32)
        asv_is_target = asv_is_target == "True"

        cm_is_target, cm_scores, cm_systems = load_scorefile_and_split_to_arrays(cm_file)
        cm_scores = cm_scores.astype(np.float32)
        cm_is_target = cm_is_target == "True"

        asv_scores_list.append(asv_scores)
        cm_scores_list.append(cm_scores)
        asv_is_target_list.append(asv_is_target)
        cm_is_target_list.append(cm_is_target)

        num_updates.append(asv_updates)

        title = "Updates: {:<6}".format(asv_updates)
        if args.metrics:
            # Compute ASV EER, CM EER and t-DCF and include it in the plot
            # Take spoof scores as well
            asv_spoof_scores = asv_scores[asv_systems != "bonafide"]
            # Only use proper ASV samples (non-spoof) to determine
            # EER for ASV
            asv_is_target_bonafide = asv_is_target[asv_systems == "bonafide"]
            asv_scores = asv_scores[asv_systems == "bonafide"]
            asv_target_scores = asv_scores[asv_is_target_bonafide]
            asv_nontarget_scores = asv_scores[~asv_is_target_bonafide]

            asv_frr, asv_far, asv_thresholds = tdcf.compute_det(
                asv_target_scores,
                asv_nontarget_scores
            )
            asv_frr_eer, asv_far_eer, asv_eer_threshold = tdcf.compute_eer(
                asv_frr,
                asv_far,
                asv_thresholds
            )
            asv_eer = (asv_frr_eer + asv_far_eer) / 2

            cm_frr, cm_far, cm_thresholds = tdcf.compute_det(
                cm_scores[cm_is_target],
                cm_scores[~cm_is_target]
            )
            cm_frr_eer, cm_far_eer, cm_eer_threshold = tdcf.compute_eer(
                cm_frr,
                cm_far,
                cm_thresholds
            )
            cm_eer = (cm_frr_eer + cm_far_eer) / 2

            tDCF_norm, cm_thresholds = tdcf.compute_asvspoof_tDCF(
                asv_target_scores,
                asv_nontarget_scores,
                asv_spoof_scores,
                cm_scores[cm_is_target],
                cm_scores[~cm_is_target],
                tdcf.ASVSPOOF2019_COST_MODEL
            )

            min_tdcf = np.min(tDCF_norm)

            # Add metrics to the title
            title += "  ASV EER: {:1.3f}  CM EER: {:1.3f}  min-t-DCF: {:1.5f}".format(
                asv_eer, cm_eer, min_tdcf
            )

            # Store metrics for later plotting
            asv_eers.append(asv_eer)
            cm_eers.append(cm_eer)
            min_tdcfs.append(min_tdcf)

        titles_list.append(title)

    if not args.quick:
        plot_joint_score_scatter_animation(
            asv_scores_list,
            cm_scores_list,
            asv_is_target_list,
            cm_is_target_list,
            titles_list,
            args.output,
            fps=5,
        )

    snapshot_filename = ".".join(args.output.split(".")[:-1])
    snapshot_filename += "_snapshots.png"

    plot_joint_score_scatter_animation_snapshots(
        asv_scores_list,
        cm_scores_list,
        asv_is_target_list,
        cm_is_target_list,
        titles_list,
        snapshot_filename,
        num_snapshots=5,
    )

    if not args.quick:
        # Render DET animations to a different file
        det_filename = ".".join(args.output.split(".")[:-1])
        det_filename += "_dets.mp4"

        plot_joint_det_animation(
            asv_scores_list,
            cm_scores_list,
            asv_is_target_list,
            cm_is_target_list,
            titles_list,
            det_filename,
            fps=5,
        )

    # Render DET change into one file
    det_filename = ".".join(args.output.split(".")[:-1])
    det_filename += "_det_change.pdf"

    plot_joint_det_change(
        asv_scores_list,
        cm_scores_list,
        asv_is_target_list,
        cm_is_target_list,
        titles_list,
        det_filename,
    )


    # Also plot metrics in one plot if those were calculated
    if args.metrics:
        # Remove filename postfix and replace it with something
        # suitable for images
        plot_filename = ".".join(args.output.split(".")[:-1])
        plot_filename += "_metrics.png"

        plot_joint_score_training_progress(
            num_updates,
            asv_eers,
            cm_eers,
            min_tdcfs,
            plot_filename
        )


def plot_average_metrics(remaining_args):
    """
    Read bunch of asv/cm score-files from different repetitions
    of the experiment, and plot the metric curves

    Note: This assumes score directories generated by the other scripts,
          e.g. train_xvector_asv.py and train_siamese_cm.py.
          The stored score files have name of form
          [experiment_name]_updates_[int]_[asv/cm]_model.txt.[asv/cm]_scores
    """
    from utils.plot_utils import (
        plot_joint_score_scatter_animation,
        plot_joint_score_training_progress,
        plot_joint_score_training_progress_single_axis
    )
    from utils.score_loading import load_scorefile_and_split_scores, load_scorefile_and_split_to_arrays
    from utils import tdcf

    parser = argparse.ArgumentParser("Plot asv/cm score metrics over repetitions")
    parser.add_argument("asv_score_dir", type=str, help="Directory where ASV scores are stored")
    parser.add_argument("cm_score_dir", type=str, help="Directory where CM scores are stored")
    parser.add_argument("output", help="Location where the output should go")
    parser.add_argument("--experiment-names", type=str, nargs="+", help="Experiment name (the prefix of the files)")
    parser.add_argument("--relative", action="store_true", help="Show results relative to the performance at the beginning")
    parser.add_argument("--single-axis", action="store_true", help="Show results in a single-axis plot rather than multi-axis one")
    parser.add_argument("--for-paper", action="store_true", help="Paper-specific tuning")
    parser.add_argument("--switch-scores", action="store_true", help="Switch scores between ASV and CM systems")
    args = parser.parse_args(remaining_args)

    assert len(args.experiment_names) > 0, "experiment_names is empty. Provide names of experiments to average over!"

    # Example filename of one scorefile for asv
    # joint_rl_asv_11_cm_3_updates_0_asv_model.txt.asv_scores

    # Gather eers/t-DCFs af all points in all experiments
    # These will be of shape [num_experiments, num_steps]
    asv_eers_repetitions = []
    cm_eers_repetitions = []
    min_tdcfs_repetitions = []
    # This tell number of updates for each point in EER/TDCFs lists
    # Will be shape of [num_experiments, num_steps]
    num_updates = None

    # Go over experiments, get the files, read the scores and compute eers etc
    for experiment_name in args.experiment_names:
        asv_files = glob(os.path.join(args.asv_score_dir, experiment_name) + "_updates_*asv_scores")
        cm_files = glob(os.path.join(args.cm_score_dir, experiment_name) + "_updates_*cm_scores")

        assert len(asv_files) > 0, "Could not find any ASV scorefiles under this experiment name"
        assert len(cm_files) > 0, "Could not find any CM scorefiles under this experiment name"
        assert len(asv_files) == len(cm_files), "Found un-equal amount of ASV and CM scorefiles"

        # Sort by how far files were in the training
        asv_files_updates = [int(re.findall("updates\_([0-9]*)", filepath)[0]) for filepath in asv_files]
        cm_files_updates = [int(re.findall("updates\_([0-9]*)", filepath)[0]) for filepath in cm_files]

        # Combine filenames with the number of updates and sort
        asv_files = list(sorted(zip(asv_files_updates, asv_files), key=lambda x: x[0]))
        cm_files = list(sorted(zip(cm_files_updates, cm_files), key=lambda x: x[0]))

        asv_eers = []
        cm_eers = []
        min_tdcfs = []

        if num_updates is None:
            num_updates = [updates_and_files[0] for updates_and_files in asv_files]

        for (asv_updates, asv_file), (cm_updates, cm_file) in tqdm(zip(asv_files, cm_files), desc="load", total=len(asv_files)):
            # Check that asv and cm file match
            assert asv_updates == cm_updates, "Something went wonky with matching ASV and CM files"

            asv_is_target, asv_scores, asv_systems = load_scorefile_and_split_to_arrays(asv_file)
            asv_scores = asv_scores.astype(np.float32)
            asv_is_target = asv_is_target == "True"

            cm_is_target, cm_scores, cm_systems = load_scorefile_and_split_to_arrays(cm_file)
            cm_scores = cm_scores.astype(np.float32)
            cm_is_target = cm_is_target == "True"

            # Swap ASV and CM scores to see how good they
            # do in each others' tasks
            if args.switch_scores:
                temp = cm_scores
                cm_scores = asv_scores
                asv_scores = temp

            # Compute ASV EER, CM EER and t-DCF and include it in the plot
            # Take spoof scores as well
            asv_spoof_scores = asv_scores[asv_systems != "bonafide"]
            # Only use proper ASV samples (non-spoof) to determine
            # EER for ASV
            asv_is_target_bonafide = asv_is_target[asv_systems == "bonafide"]
            asv_scores = asv_scores[asv_systems == "bonafide"]
            asv_target_scores = asv_scores[asv_is_target_bonafide]
            asv_nontarget_scores = asv_scores[~asv_is_target_bonafide]

            asv_frr, asv_far, asv_thresholds = tdcf.compute_det(
                asv_target_scores,
                asv_nontarget_scores
            )
            asv_frr_eer, asv_far_eer, asv_eer_threshold = tdcf.compute_eer(
                asv_frr,
                asv_far,
                asv_thresholds
            )
            asv_eer = (asv_frr_eer + asv_far_eer) / 2

            cm_frr, cm_far, cm_thresholds = tdcf.compute_det(
                cm_scores[cm_is_target],
                cm_scores[~cm_is_target]
            )
            cm_frr_eer, cm_far_eer, cm_eer_threshold = tdcf.compute_eer(
                cm_frr,
                cm_far,
                cm_thresholds
            )
            cm_eer = (cm_frr_eer + cm_far_eer) / 2

            tDCF_norm, cm_thresholds = tdcf.compute_asvspoof_tDCF(
                asv_target_scores,
                asv_nontarget_scores,
                asv_spoof_scores,
                cm_scores[cm_is_target],
                cm_scores[~cm_is_target],
                tdcf.ASVSPOOF2019_COST_MODEL
            )

            min_tdcf = np.min(tDCF_norm)

            # Store metrics for later plotting
            asv_eers.append(asv_eer)
            cm_eers.append(cm_eer)
            min_tdcfs.append(min_tdcf)

        asv_eers_repetitions.append(asv_eers)
        cm_eers_repetitions.append(cm_eers)
        min_tdcfs_repetitions.append(min_tdcfs)

    # Compute averages over repetitions
    asv_eers_repetitions = np.array(asv_eers_repetitions)
    cm_eers_repetitions = np.array(cm_eers_repetitions)
    min_tdcfs_repetitions = np.array(min_tdcfs_repetitions)

    # Changes relative to the starting model, if desired
    if args.relative:
        # Scale
        asv_eers_repetitions /= asv_eers_repetitions[:, 0, None]
        cm_eers_repetitions /= cm_eers_repetitions[:, 0, None]
        min_tdcfs_repetitions /= min_tdcfs_repetitions[:, 0, None]
        # Center
        asv_eers_repetitions -= 1.0
        cm_eers_repetitions -= 1.0
        min_tdcfs_repetitions -= 1.0
        # Multiply by 100 (for percentages)
        asv_eers_repetitions *= 100.0
        cm_eers_repetitions *= 100.0
        min_tdcfs_repetitions *= 100.0

    mean_asv_eers = np.mean(asv_eers_repetitions, axis=0)
    mean_cm_eers = np.mean(cm_eers_repetitions, axis=0)
    mean_min_tdcfs = np.mean(min_tdcfs_repetitions, axis=0)

    std_asv_eers = np.std(asv_eers_repetitions, axis=0)
    std_cm_eers = np.std(cm_eers_repetitions, axis=0)
    std_min_tdcfs = np.std(min_tdcfs_repetitions, axis=0)

    if args.single_axis:
        plot_joint_score_training_progress_single_axis(
            num_updates,
            mean_asv_eers,
            mean_cm_eers,
            mean_min_tdcfs,
            args.output,
            asv_eers_stds=std_asv_eers,
            cm_eers_stds=std_cm_eers,
            min_tdcfs_stds=std_min_tdcfs,
            for_paper=args.for_paper,
        )
    else:
        plot_joint_score_training_progress(
            num_updates,
            mean_asv_eers,
            mean_cm_eers,
            mean_min_tdcfs,
            args.output,
            asv_eers_stds=std_asv_eers,
            cm_eers_stds=std_cm_eers,
            min_tdcfs_stds=std_min_tdcfs,
        )


# Dictionary mapping short names to functions to be called
AVAILABLE_COMMANDS = {
    "embedding": plot_embedding,
    "score-animation": plot_score_animation,
    "average-metrics": plot_average_metrics
}

if __name__ == "__main__":
    parser = argparse.ArgumentParser("Main plotting script")
    parser.add_argument(
        "command",
        type=str,
        choices=list(AVAILABLE_COMMANDS.keys()),
        help="Plotting operation to do"
    )
    args, remaining_args = parser.parse_known_args()
    AVAILABLE_COMMANDS[args.command](remaining_args)
